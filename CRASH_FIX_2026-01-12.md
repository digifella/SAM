# WSL Crash Fix - January 12, 2026

## What Happened

The `run_sam_interactive.py` script caused a WSL crash due to an **infinite processing loop**.

### Root Cause

The script was processing audio files in a feedback loop:
1. Process `file.wav` from input directory
2. Save `file_target.wav` and `file_residual.wav` to output directory
3. Output directory was same as (or overlapping with) input directory
4. Script detected `file_target.wav` as a new input file
5. Processed it again → `file_target_target.wav`, `file_target_residual.wav`
6. Loop continued until system resources exhausted → WSL crash

### Evidence from Screenshot

Left terminal showed repeated processing of `Device.Speech_audio_processed.wav` - the word "processed" in the filename indicates it was already an output file being reprocessed.

## Fixes Applied

### 1. Skip Already-Processed Files (run_sam_interactive.py:360-372)

```python
def find_audio_files(input_dir: Path) -> List[Path]:
    """Find all audio files in input directory, excluding already-processed files."""
    # ...
    excluded_patterns = ['_target', '_residual', '_processed', '_separated']

    for f in audio_files:
        if any(pattern in f.stem.lower() for pattern in excluded_patterns):
            print(f"  ⊗ Skipping already-processed file: {f.name}")
            continue
```

This prevents files with output-like names from being processed as inputs.

### 2. Directory Collision Detection (run_sam_interactive.py:822-857)

```python
# CRITICAL FIX: Prevent directory collision
input_resolved = input_dir.resolve()
output_resolved = output_dir.resolve()

if input_resolved == output_resolved:
    print("ERROR: Input and output directories CANNOT be the same!")
    # ... detailed error message ...
    sys.exit(1)
```

This blocks execution if input and output directories are identical, with a clear error message explaining the infinite loop risk.

### 3. Session-Based File Tracking (run_sam_interactive.py:894-904)

```python
processed_files = set()  # Track processed files

for audio_file in audio_files:
    file_id = audio_file.resolve()
    if file_id in processed_files:
        print(f"\n⊗ Skipping {audio_file.name} (already processed in this session)")
        continue
    processed_files.add(file_id)
```

Prevents the same file from being processed twice in one session, even if it appears multiple times in the file list.

### 4. Maximum File Limit (run_sam_interactive.py:49, 870-885)

```python
MAX_FILES_PER_SESSION = 100  # Prevent runaway processing

if len(audio_files) > MAX_FILES_PER_SESSION:
    print(f"WARNING: Too many files detected ({len(audio_files)} files)")
    # ... prompt user to confirm or cancel ...
```

Hard limit on files per session. If more than 100 files are detected (likely indicates a feedback loop), the script requires user confirmation.

## Testing the Fix

To verify the fixes work:

```bash
# Test 1: Try to use same directory for input and output (should fail)
python run_sam_interactive.py
# Enter same path for input and output - should see error and exit

# Test 2: Place a file with "_target" in name in input dir (should skip)
touch Audio_input/test_target.wav
python run_sam_interactive.py
# Should see: "⊗ Skipping already-processed file: test_target.wav"

# Test 3: Large number of files (should prompt)
# Create 150 dummy files - should see warning and prompt
```

## Prevention Checklist

Before running the script, verify:
- [ ] Input directory != Output directory
- [ ] Input directory is not a parent/child of output directory
- [ ] Output files are not being copied/moved back to input directory
- [ ] No automated scripts are watching the directories and moving files
- [ ] Reasonable number of files in input directory (<100)

## Safe Usage

**Correct setup:**
```
input_dir:  /home/user/audio_input
output_dir: /home/user/audio_output
```

**DANGEROUS setups:**
```
# Both same directory
input_dir:  /home/user/audio
output_dir: /home/user/audio

# Output inside input
input_dir:  /home/user/audio
output_dir: /home/user/audio/output  # Will still see output files!

# Symlink to same location
input_dir:  /home/user/audio_in
output_dir: /mnt/drive/audio  # But /mnt/drive is symlinked to /home/user/audio_in
```

## Related Files

- Main script: `run_sam_interactive.py`
- Config file: `~/.sam_audio_config.json`
- Log directory: `~/.sam_audio_logs/`
- Screenshot: `Screenshot 2026-01-12 104121.jpg`

## Status

⚠️ **PARTIALLY FIXED** - Directory collision safeguards work, but **memory exhaustion issue persists**.

## NEW ISSUE DISCOVERED (2026-01-12, 11:17 AM)

### Problem: Memory Exhaustion During Chunking

The script still crashes with "Killed" during audio processing due to excessive memory usage.

### Root Cause Analysis

**Issue 1: All chunks loaded into RAM at once (run_sam_interactive.py:663)**

```python
chunks = chunk_audio(process_path, chunk_duration, overlap)
```

The `chunk_audio` function loads the entire audio file and splits it into chunks, storing ALL chunk data in memory simultaneously:

```python
def chunk_audio(audio_path, chunk_duration, overlap):
    chunks = []
    # ... loop ...
    audio_data, sr = sf.read(audio_path, start=start_frame, frames=num_frames)
    chunks.append((start, end, audio_data, sr))  # ALL chunks accumulate in memory!
    return chunks
```

**Issue 2: Processed chunks also accumulate (run_sam_interactive.py:684-685)**

```python
target_chunks.append(target)
residual_chunks.append(residual)
```

This triples memory usage: original chunks + target chunks + residual chunks.

**Issue 3: GPU memory pressure**

Model uses 30.77 GB / 48 GB GPU memory from the start. Processing chunks requires additional GPU memory for tensors, leaving little headroom.

### Memory Calculation Example

For 300-second audio (like Denise_Speech_audio_processed.wav):
- Audio: 300s × 16,000 Hz × 4 bytes = 19.2 MB
- Split into 11 chunks (30s each, 2s overlap)
- Original chunks in memory: 19.2 MB
- Target chunks in memory: 19.2 MB
- Residual chunks in memory: 19.2 MB
- **Total base: ~60 MB**
- **With overhead (tensors, intermediate buffers, temp conversions): Can exceed available RAM**

The WSL2 OOM killer terminates the process when system RAM is exhausted.

### Required Fix

**Convert to streaming/iterator pattern:**

Instead of loading all chunks at once:
```python
# CURRENT (loads all chunks into memory)
chunks = chunk_audio(audio_path, chunk_duration, overlap)
for chunk in chunks:
    process(chunk)
```

Use a generator that yields one chunk at a time:
```python
# FIXED (streams one chunk at a time)
for chunk in chunk_audio_generator(audio_path, chunk_duration, overlap):
    target, residual = process(chunk)
    # Save or accumulate results
    # Free chunk memory immediately
```

This would reduce peak memory usage from "all chunks + all results" to "one chunk + accumulated results".

## MEMORY FIX IMPLEMENTED (2026-01-12, 11:20 AM)

### Changes Made to run_sam_interactive.py

**1. Created generator function (lines 435-465)**

```python
def chunk_audio_generator(audio_path: Path, chunk_duration: float, overlap: float):
    """
    Generator that yields audio chunks one at a time to minimize memory usage.
    Yields (start_time, end_time, chunk_data, sample_rate) tuples.
    """
    # ... yields chunks one at a time instead of accumulating in list
```

**2. Added chunk counting function (lines 468-485)**

```python
def count_chunks(audio_path: Path, chunk_duration: float, overlap: float) -> int:
    """
    Calculate the number of chunks without loading audio into memory.
    """
    # ... calculates chunk count for progress display
```

**3. Updated processing loop (lines 678-728)**

- Replaced `chunk_audio()` with `chunk_audio_generator()`
- Use `count_chunks()` to get total count for progress display
- Stream chunks one at a time: only ONE chunk in memory at any time
- Capture sample_rate from first chunk instead of pre-loading
- Removed reference to `chunks` list in cleanup

### Memory Impact

**Before (old approach):**
- Load entire file → split into 11 chunks → all chunks in memory
- Process each chunk → accumulate target and residual
- Peak memory: original (19 MB) + targets (19 MB) + residuals (19 MB) = ~60 MB base + overhead

**After (streaming approach):**
- Stream one chunk at a time from disk
- Only current chunk + accumulated results in memory
- Peak memory: 1 chunk (1.7 MB) + accumulated results (~38 MB) = much lower peak

**Key improvement:** Original chunks are no longer kept in memory - they're read on-demand and immediately freed after processing.

### Testing

Ready to test with the problematic 300-second audio file.

## CRITICAL BUG FIX (2026-01-12, 1:25 PM)

### Infinite Loop Bug Discovered

**Problem:** The initial implementation had an infinite loop in `count_chunks()` and `chunk_audio_generator()`.

**Buggy code (lines 481, 464):**
```python
start = end - overlap  # BUG: Goes backwards when end=total_duration
if start >= total_duration:
    break
```

When processing the last chunk where `end = total_duration`, the calculation `start = total_duration - overlap` produces a value less than `total_duration`, causing the loop to continue forever.

**Symptom:** Process stuck at 103% CPU for nearly 2 hours with no progress after "Using chunking" log message.

**Fix:** Always move forward by the non-overlapping portion:
```python
step = chunk_duration - overlap
start += step  # Always advances forward
```

Now the loop naturally terminates when `start >= total_duration`.

### Status

✅ **FIXED** - Infinite loop bug resolved. Ready for testing.

## SUCCESS - Fix Verified (2026-01-12, 1:46 PM)

### Test Results

Successfully processed the problematic 300-second audio file without crashing!

**Processing stats:**
- Input: Denise_Speech_audio_processed.wav (300.39 seconds)
- Chunks: 11 chunks (30s each, 2s overlap)
- Processing time: ~20 seconds per chunk
- Total time: ~3-4 minutes (after compilation)
- Memory usage: **Stable at 12.5 GB CPU RAM throughout** (no explosion!)
- GPU memory: Stable at 31 GB (no OOM)

**Key improvements confirmed:**
1. ✅ Streaming generator works - only one chunk in memory at a time
2. ✅ No memory explosion - stayed flat at 32% RAM usage
3. ✅ No WSL2 crash - process completed successfully
4. ✅ Output files created successfully

**Output files:**
- /mnt/f/hf-home/audio_output/Denise_Speech_audio_processed_target.wav
- /mnt/f/hf-home/audio_output/Denise_Speech_audio_processed_residual.wav

### Final Status

✅ **FULLY RESOLVED** - Memory streaming fix works perfectly. The script can now process long audio files without exhausting system memory.
